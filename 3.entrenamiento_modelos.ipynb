{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se forzó a realizar el entrenamiento con un hilo porque los modelos estaban entrenando con todos los hilos incluso cuando se les especificaba que usaran solo 1 o 2\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Librerías básicas\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "\n",
    "# Librerías de machine learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Librerías para encoding\n",
    "import pywt\n",
    "from scipy.fft import fft\n",
    "from itertools import product\n",
    "\n",
    "# Configurar pandas\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Variables compartidas\n",
    "N_CORES = 1\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Crear directorios necesarios\n",
    "os.makedirs(\"modelos/SVM\", exist_ok=True)\n",
    "os.makedirs(\"modelos/RandomForest\", exist_ok=True) \n",
    "os.makedirs(\"modelos/XGBoost\", exist_ok=True)\n",
    "os.makedirs(\"results/training_results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_with_time(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def save_model_safely(model, model_name, encoding_name, algorithm, metrics, params):\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    model_dir = f\"modelos/{algorithm}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Nombre del archivo\n",
    "    filename = f\"{model_dir}/{encoding_name}.joblib\"\n",
    "    metadata_file = f\"{model_dir}/{encoding_name}_metadata.json\"\n",
    "    \n",
    "    try:\n",
    "        # Guardar modelo\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"\\tModelo guardado: {filename}\")\n",
    "        \n",
    "        # Guardar metadata\n",
    "        metadata = {\n",
    "            'model_name': model_name,\n",
    "            'encoding': encoding_name,\n",
    "            'algorithm': algorithm,\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'f1_score_weighted': float(metrics['f1_score_weighted']),\n",
    "            'f1_score_macro': float(metrics['f1_score_macro']),\n",
    "            'parameters': params,\n",
    "            'model_file': filename\n",
    "        }\n",
    "        \n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        return filename, metadata_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_with_time(f\"Error guardando modelo: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_training_results(results, filename):\n",
    "    \"\"\"Guarda resultados de entrenamiento para evaluación posterior\"\"\"\n",
    "    filepath = f\"results/training_results/{filename}\"\n",
    "    try:\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\tResultados guardados: {filepath}\")\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        log_with_time(f\"Error guardando resultados: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7e3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(sequences, is_str=True):\n",
    "    if is_str:\n",
    "        templist=[]\n",
    "        for seq in sequences:\n",
    "            num_seq=[ord(char) for char in seq]\n",
    "            fft_seq=fft(num_seq)\n",
    "            fft_seq=np.abs(fft_seq)\n",
    "            templist.append(fft_seq[1:len(fft_seq)//2])\n",
    "        return templist\n",
    "    else:\n",
    "        templist=[]\n",
    "        for seq in sequences:\n",
    "            fft_seq=fft(seq)\n",
    "            fft_seq=np.abs(fft_seq)\n",
    "            templist.append(fft_seq[1:len(fft_seq)//2])\n",
    "        return templist\n",
    "\n",
    "def generate_kmers_dict(k, unique_chars=set('ACGNT')):\n",
    "    kmers = product(unique_chars, repeat=k)\n",
    "    kmer_dict = {''.join(kmer): i for i,kmer in enumerate(kmers)}\n",
    "    return kmer_dict\n",
    "\n",
    "def k_mers(sequencias, k=3, unique_chars=set('ACGNT')):\n",
    "    kmers_map=generate_kmers_dict(k, unique_chars)\n",
    "    templist=[]\n",
    "    for seq in sequencias:\n",
    "        temp=[seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
    "        templist.append([kmers_map[i] for i in temp])\n",
    "    return templist\n",
    "\n",
    "def one_hot(sequences, max_len, unique_chars=set('ACGNT'), reshape=True):\n",
    "    mapping={j:i for i,j in enumerate(unique_chars)}\n",
    "    sequencias_procesadas=[]\n",
    "    if reshape==True:\n",
    "        for s in sequences:\n",
    "            temp=np.zeros((max_len,len(unique_chars)))\n",
    "            for c in zip(s,temp):\n",
    "                    c[1][mapping[c[0]]]=1\n",
    "            sequencias_procesadas.append(temp.reshape(-1))\n",
    "        return sequencias_procesadas\n",
    "    elif reshape==False:\n",
    "        for s in sequences:\n",
    "            temp=np.zeros((max_len,len(unique_chars)))\n",
    "            for c in zip(s,temp):\n",
    "                    c[1][mapping[c[0]]]=1\n",
    "            sequencias_procesadas.append(temp)\n",
    "        return sequencias_procesadas\n",
    "\n",
    "def wavelet(sequences, numeric=False, wavelet='db1', level=5):\n",
    "    templist=[]\n",
    "    if numeric==False:\n",
    "        for seq in sequences:\n",
    "            num_seq=[ord(char) for char in seq]\n",
    "            coeffs=pywt.wavedec(num_seq, wavelet, level)\n",
    "            templist.append(np.concatenate(coeffs))\n",
    "        return templist\n",
    "    elif numeric==True:\n",
    "        for seq in sequences:\n",
    "            coeffs=pywt.wavedec(seq, wavelet, level)\n",
    "            templist.append(np.concatenate(coeffs))\n",
    "        return templist\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < maxlen:\n",
    "            seq += 'N' * (maxlen - len(seq))  \n",
    "        else:\n",
    "            seq = seq[:maxlen]\n",
    "        padded_sequences.append(seq)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f5aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred, y_score, classes):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_score_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "        'precision_weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'precision_macro': precision_score(y_test, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    n_classes = len(classes)\n",
    "    class_metrics = {\n",
    "        'sensitivity_per_class': {},\n",
    "        'specificity_per_class': {},\n",
    "        'precision_per_class': {},\n",
    "        'recall_per_class': {}\n",
    "    }\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        TP = cm[i, i]\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        TN = np.sum(cm) - (TP + FP + FN)\n",
    "        \n",
    "        class_metrics['sensitivity_per_class'][classes[i]] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        class_metrics['specificity_per_class'][classes[i]] = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        class_metrics['precision_per_class'][classes[i]] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        class_metrics['recall_per_class'][classes[i]] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    metrics.update(class_metrics)\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model_simple(model, X_test, y_test, class_names):\n",
    "    \"\"\"Evaluación simple sin generar gráficos\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Obtener las clases numéricas únicas\n",
    "    classes_num = np.unique(y_test)\n",
    "    # Mapear las clases numéricas a nombres reales\n",
    "    classes = [class_names[i] for i in classes_num]\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, y_pred, y_score, classes)\n",
    "    \n",
    "    # Preparar datos para evaluación posterior\n",
    "    evaluation_data = {\n",
    "        'y_true': y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test),\n",
    "        'y_pred': y_pred.tolist(),\n",
    "        'y_proba': y_score.tolist(),\n",
    "        'classes': classes,\n",
    "        'class_names_mapping': class_names\n",
    "    }\n",
    "    \n",
    "    return metrics, evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f003db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, X_test, y_train, y_test, encoding_name, class_names, \n",
    "                      kernel='rbf', C=1.0, gamma='scale'):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = SVC(probability=True, kernel=kernel, C=C, gamma=gamma)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    metrics, evaluation_data = evaluate_model_simple(model, X_test, y_test, class_names)\n",
    "    \n",
    "    best_params = {'kernel': kernel, 'C': C, 'gamma': gamma}\n",
    "    \n",
    "    print(f\"\\tSVM {encoding_name} - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score_weighted']:.4f}, Tiempo: {training_time:.2f}s\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_file, metadata_file = save_model_safely(\n",
    "        model, 'SVM', encoding_name, 'SVM', metrics, best_params\n",
    "    )\n",
    "    \n",
    "    # Preparar resultados completos\n",
    "    results = {\n",
    "        'model_name': 'SVM',\n",
    "        'encoding': encoding_name,\n",
    "        'metrics': metrics,\n",
    "        'parameters': best_params,\n",
    "        'training_time': training_time,\n",
    "        'model_file': model_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, encoding_name, class_names,\n",
    "                                n_estimators=100, max_depth=None, min_samples_split=2, \n",
    "                                n_cores=1, random_state=42):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=random_state,\n",
    "        n_jobs=1,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    metrics, evaluation_data = evaluate_model_simple(model, X_test, y_test, class_names)\n",
    "    \n",
    "    best_params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split\n",
    "    }\n",
    "    \n",
    "    print(f\"\\tRandom Forest {encoding_name} - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score_weighted']:.4f}, Tiempo: {training_time:.2f}s\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_file, metadata_file = save_model_safely(\n",
    "        model, 'Random Forest', encoding_name, 'RandomForest', metrics, best_params\n",
    "    )\n",
    "    \n",
    "    # Preparar resultados completos\n",
    "    results = {\n",
    "        'model_name': 'Random Forest',\n",
    "        'encoding': encoding_name,\n",
    "        'metrics': metrics,\n",
    "        'parameters': best_params,\n",
    "        'training_time': training_time,\n",
    "        'model_file': model_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, encoding_name, class_names,\n",
    "                          n_estimators=100, max_depth=6, learning_rate=0.3, \n",
    "                          subsample=0.8, colsample_bytree=0.8, n_cores=1, random_state=42):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        tree_method='hist',\n",
    "        grow_policy='lossguide',\n",
    "        random_state=random_state,\n",
    "        n_jobs=1,       \n",
    "        verbosity=0,    \n",
    "        nthread=1,      \n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    metrics, evaluation_data = evaluate_model_simple(model, X_test, y_test, class_names)\n",
    "    \n",
    "    best_params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree\n",
    "    }\n",
    "    \n",
    "    print(f\"\\tXGBoost {encoding_name} - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score_weighted']:.4f}, Tiempo: {training_time:.2f}s\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_file, metadata_file = save_model_safely(\n",
    "        model, 'XGBoost', encoding_name, 'XGBoost', metrics, best_params\n",
    "    )\n",
    "    \n",
    "    # Preparar resultados completos\n",
    "    results = {\n",
    "        'model_name': 'XGBoost',\n",
    "        'encoding': encoding_name,\n",
    "        'metrics': metrics,\n",
    "        'parameters': best_params,\n",
    "        'training_time': training_time,\n",
    "        'model_file': model_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0da23d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Datos listos\n"
     ]
    }
   ],
   "source": [
    "print('Cargando datos...')\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv('datos/datos_filtrados_sin_encoding.csv')\n",
    "df = df.rename(columns={'sequence': 'aligned_sequence'})\n",
    "\n",
    "print(\"Datos listos\")\n",
    "\n",
    "# Padding de secuencias\n",
    "maxlen = max([len(i) for i in df['original_sequence']]) \n",
    "df['padded_sequences'] = pad_sequences(df['original_sequence'], maxlen)\n",
    "\n",
    "# Calcular longitudes\n",
    "df['len_ps'] = [len(i) for i in df['padded_sequences']]\n",
    "df['len_as'] = [len(i) for i in df['aligned_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae586cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificando secuencias\n",
      "Secuencias codificadas\n"
     ]
    }
   ],
   "source": [
    "print(\"Codificando secuencias\")\n",
    "\n",
    "# Aligned Sequences (AS)\n",
    "df['AS_One Hot'] = one_hot(df['aligned_sequence'].values, len(df['aligned_sequence'][0]))\n",
    "df['AS_K-mers'] = k_mers(df['aligned_sequence'].values)\n",
    "df['AS_FFT'] = fourier(df['aligned_sequence'].values)\n",
    "df['AS_Wavelet'] = wavelet(df['aligned_sequence'].values)\n",
    "df['AS_K-mers + FFT'] = fourier(df['AS_K-mers'].values, False)\n",
    "df['AS_One Hot + FFT'] = fourier(df['AS_One Hot'].values, False)\n",
    "df['AS_K-mers + Wavelet'] = wavelet(df['AS_K-mers'].values, True)\n",
    "df['AS_One Hot + Wavelet'] = wavelet(df['AS_One Hot'].values, True)\n",
    "\n",
    "# Padded Sequences (PS)\n",
    "df['PS_One Hot'] = one_hot(df['padded_sequences'].values, len(df['padded_sequences'][0]))\n",
    "df['PS_K-mers'] = k_mers(df['padded_sequences'].values)\n",
    "df['PS_FFT'] = fourier(df['padded_sequences'].values)\n",
    "df['PS_Wavelet'] = wavelet(df['padded_sequences'].values)\n",
    "df['PS_K-mers + FFT'] = fourier(df['PS_K-mers'].values, False)\n",
    "df['PS_One Hot + FFT'] = fourier(df['PS_One Hot'].values, False)\n",
    "df['PS_K-mers + Wavelet'] = wavelet(df['PS_K-mers'].values, True)\n",
    "df['PS_One Hot + Wavelet'] = wavelet(df['PS_One Hot'].values, True)\n",
    "\n",
    "print(\"Secuencias codificadas\")\n",
    "\n",
    "# Cargar mapeo de clases\n",
    "mapeo_df = pd.read_csv('datos/mapeo_clases.csv')\n",
    "df = df.merge(mapeo_df.rename(columns={'model_class': 'clases_modelos'}), \n",
    "              on='genus', how='left')\n",
    "\n",
    "# Crear mapeo inverso para nombres de clases\n",
    "tempdf = df[['genus', 'clases_modelos']].drop_duplicates()\n",
    "reverse_map_genus = {v2: v1 for v1, v2 in tempdf.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea31d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = [\n",
    "    'AS_One Hot', \n",
    "    'AS_K-mers', \n",
    "    'AS_FFT',\n",
    "    'AS_Wavelet', \n",
    "    'AS_K-mers + FFT', \n",
    "    'AS_One Hot + FFT',\n",
    "    'AS_K-mers + Wavelet', \n",
    "    'AS_One Hot + Wavelet', \n",
    "    'PS_One Hot',\n",
    "    'PS_K-mers', \n",
    "    'PS_FFT', \n",
    "    'PS_Wavelet', \n",
    "    'PS_K-mers + FFT',\n",
    "    'PS_One Hot + FFT', \n",
    "    'PS_K-mers + Wavelet', \n",
    "    'PS_One Hot + Wavelet'\n",
    "]\n",
    "\n",
    "# Parámetros óptimos del código original\n",
    "parametros_optimos = {\n",
    "    'SVM': {\n",
    "        'AS_One Hot': {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'},\n",
    "        'AS_K-mers': {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_FFT': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_Wavelet': {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_K-mers + FFT': {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_One Hot + FFT': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_K-mers + Wavelet': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'AS_One Hot + Wavelet': {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'},\n",
    "        'PS_One Hot': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_K-mers': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_FFT': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_Wavelet': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_K-mers + FFT': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_One Hot + FFT': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'},\n",
    "        'PS_K-mers + Wavelet': {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'},\n",
    "        'PS_One Hot + Wavelet': {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'AS_One Hot': {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100},\n",
    "        'AS_K-mers': {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 100},\n",
    "        'AS_FFT': {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100},\n",
    "        'AS_Wavelet': {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50},\n",
    "        'AS_K-mers + FFT': {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100},\n",
    "        'AS_One Hot + FFT': {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200},\n",
    "        'AS_K-mers + Wavelet': {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 100},\n",
    "        'AS_One Hot + Wavelet': {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 50},\n",
    "        'PS_One Hot': {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "        'PS_K-mers': {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 200},\n",
    "        'PS_FFT': {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200},\n",
    "        'PS_Wavelet': {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200},\n",
    "        'PS_K-mers + FFT': {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200},\n",
    "        'PS_One Hot + FFT': {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200},\n",
    "        'PS_K-mers + Wavelet': {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 50},\n",
    "        'PS_One Hot + Wavelet': {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'AS_One Hot': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_K-mers': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_FFT': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_Wavelet': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_K-mers + FFT': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_One Hot + FFT': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_K-mers + Wavelet': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'AS_One Hot + Wavelet': {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_One Hot': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_K-mers': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_FFT': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_Wavelet': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_K-mers + FFT': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_One Hot + FFT': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_K-mers + Wavelet': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8},\n",
    "        'PS_One Hot + Wavelet': {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd1f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 3514 muestras\n",
      "Evaluation data: 3838 muestras\n",
      "datos de testeo guardados en: datos/evaluation_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Filtrar datos para entrenamiento (is_training=True)\n",
    "training_data = df[df['is_training'] == True].copy()\n",
    "evaluation_data = df[df['is_training'] == False].copy()\n",
    "\n",
    "print(f\"Training data: {len(training_data)} muestras\")\n",
    "print(f\"Evaluation data: {len(evaluation_data)} muestras\")\n",
    "\n",
    "# Guardar datos de evaluación para uso posterior\n",
    "test_data_path='datos/evaluation_data.parquet'\n",
    "if os.path.exists(test_data_path):\n",
    "    print(f\"datos de testeo guardados en: {test_data_path}\")\n",
    "else:\n",
    "    evaluation_data.to_parquet(test_data_path, index=False)\n",
    "    print(f\"datos de testeo guardados en: {test_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "all_results = []\n",
    "experiment_start_time = time.time()\n",
    "\n",
    "for i, enc in enumerate(encodings):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\\t-Procesando encoding {i+1}/{len(encodings)}: {enc}\")\n",
    "    \n",
    "    # Preparar datos para este encoding\n",
    "    X = training_data[enc].tolist()\n",
    "    y = training_data['clases_modelos'].values\n",
    "    \n",
    "    # División train/test para evaluación interna\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # ================== SVM ==================\n",
    "    params_svm = parametros_optimos['SVM'][enc]\n",
    "    results_svm = train_svm(\n",
    "        X_train, X_test, y_train, y_test, enc, reverse_map_genus,\n",
    "        **params_svm\n",
    "    )\n",
    "    all_results.append(results_svm)\n",
    "    \n",
    "    # Guardar resultados parciales\n",
    "    save_training_results(results_svm, f\"svm_{enc}.json\")\n",
    "    \n",
    "    # ================== Random Forest ==================\n",
    "    params_rf = parametros_optimos['Random Forest'][enc]\n",
    "    results_rf = train_random_forest(\n",
    "        X_train, X_test, y_train, y_test, enc, reverse_map_genus,\n",
    "        n_cores=N_CORES, random_state=RANDOM_STATE,\n",
    "        **params_rf\n",
    "    )\n",
    "    all_results.append(results_rf)\n",
    "    \n",
    "    # Guardar resultados parciales\n",
    "    save_training_results(results_rf, f\"rf_{enc}.json\")\n",
    "    \n",
    "    # ================== XGBoost ==================\n",
    "    params_xgb = parametros_optimos['XGBoost'][enc]\n",
    "    results_xgb = train_xgboost(\n",
    "        X_train, X_test, y_train, y_test, enc, reverse_map_genus,\n",
    "        n_cores=N_CORES, random_state=RANDOM_STATE,\n",
    "        **params_xgb\n",
    "    )\n",
    "    all_results.append(results_xgb)\n",
    "    \n",
    "    # Guardar resultados parciales\n",
    "    save_training_results(results_xgb, f\"xgb_{enc}.json\")\n",
    "    \n",
    "experiment_end_time = time.time()\n",
    "total_experiment_time = experiment_end_time - experiment_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bab15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Guardando resultados...\")\n",
    "\n",
    "# Guardar todos los resultados\n",
    "final_results = {\n",
    "    'experiment_info': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_time_seconds': total_experiment_time,\n",
    "        'total_time_minutes': total_experiment_time / 60,\n",
    "        'n_encodings': len(encodings),\n",
    "        'n_algorithms': 3,\n",
    "        'total_models': len(all_results),\n",
    "        'training_samples': len(training_data),\n",
    "        'evaluation_samples': len(evaluation_data),\n",
    "        'config': {\n",
    "            'n_cores': N_CORES,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'test_size': TEST_SIZE\n",
    "        }\n",
    "    },\n",
    "    'results': all_results,\n",
    "    'encodings': encodings,\n",
    "    'class_mapping': reverse_map_genus\n",
    "}\n",
    "\n",
    "# Guardar con timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_results_file = f\"results/training_results/final_results.json\"\n",
    "\n",
    "with open(final_results_file, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"Resultados finales guardados: {final_results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ============================================================================\n",
    "# # RESUMEN FINAL\n",
    "# # ============================================================================\n",
    "\n",
    "# log_with_time(f\"\\n{'='*60}\")\n",
    "# log_with_time(\"ENTRENAMIENTO COMPLETADO\")\n",
    "# log_with_time(f\"{'='*60}\")\n",
    "# log_with_time(f\"Tiempo total: {total_experiment_time/60:.2f} minutos\")\n",
    "# log_with_time(f\"Encodings procesados: {len(encodings)}\")\n",
    "# log_with_time(f\"Modelos entrenados: {len(all_results)}\")\n",
    "# log_with_time(f\"Datos de entrenamiento: {len(training_data)} muestras\")\n",
    "# log_with_time(f\"Datos de evaluación: {len(evaluation_data)} muestras\")\n",
    "# log_with_time(f\"Resultados guardados en: {final_results_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
