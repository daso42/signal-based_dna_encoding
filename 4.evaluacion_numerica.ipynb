{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce35fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "EVALUACIÓN NUMÉRICA COMPLETA DE MODELOS\n",
    "======================================\n",
    "\n",
    "Script para evaluación numérica detallada de modelos con cálculo exhaustivo\n",
    "de métricas, análisis de tiempos y guardado estructurado de resultados.\n",
    "\n",
    "Uso: python evaluation_numerical.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "import sys\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librerías de machine learning\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('evaluation_log.txt'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"EVALUACIÓN NUMÉRICA COMPLETA DE MODELOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuración centralizada para la evaluación\"\"\"\n",
    "    def __init__(self):\n",
    "        # Configuraciones principales\n",
    "        self.save_individual_predictions = True\n",
    "        self.save_detailed_reports = True\n",
    "        self.calculate_class_metrics = True\n",
    "        self.calculate_timing_metrics = True\n",
    "        self.calculate_statistical_analysis = True\n",
    "        \n",
    "        # Configuraciones de archivos - MODIFICADO PARA TU ESTRUCTURA\n",
    "        self.results_dir = \"results/numerical_evaluation\"\n",
    "        self.models_dir = \"models\"  # Cambiado de results/training_results\n",
    "        self.data_file = \"results/evaluation_data.parquet\"\n",
    "        \n",
    "        # Archivos alternativos para datos de evaluación\n",
    "        self.alternative_data_files = [\n",
    "            \"eval.csv\",\n",
    "            \"data/evaluation_data.csv\",\n",
    "            \"datos/evaluation_data.csv\"\n",
    "        ]\n",
    "        \n",
    "        # Métricas a calcular\n",
    "        self.metrics_to_calculate = [\n",
    "            'accuracy', 'f1_weighted', 'f1_macro', 'f1_micro',\n",
    "            'precision_weighted', 'precision_macro', 'precision_micro',\n",
    "            'recall_weighted', 'recall_macro', 'recall_micro'\n",
    "        ]\n",
    "        \n",
    "        # Configuraciones de memoria\n",
    "        self.memory_efficient = True\n",
    "        self.max_models_in_memory = 5\n",
    "\n",
    "config = EvaluationConfig()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIONES DE UTILIDAD\n",
    "# ============================================================================\n",
    "\n",
    "def log_with_time(message, level=\"INFO\"):\n",
    "    \"\"\"Log con timestamp\"\"\"\n",
    "    if level == \"INFO\":\n",
    "        logger.info(message)\n",
    "    elif level == \"ERROR\":\n",
    "        logger.error(message)\n",
    "    elif level == \"WARNING\":\n",
    "        logger.warning(message)\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    \"\"\"Asegura que el directorio existe\"\"\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_deep_size(obj, seen=None):\n",
    "    \"\"\"Calcula tamaño real de objetos anidados en MB\"\"\"\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    \n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    \n",
    "    seen.add(obj_id)\n",
    "    size = sys.getsizeof(obj)\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        size += sum(get_deep_size(k, seen) + get_deep_size(v, seen) for k, v in obj.items())\n",
    "    elif isinstance(obj, (list, tuple, set, frozenset)):\n",
    "        size += sum(get_deep_size(item, seen) for item in obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        size = obj.nbytes\n",
    "    \n",
    "    return size / (1024 * 1024)  # Convertir a MB\n",
    "\n",
    "# ============================================================================\n",
    "# DESCUBRIMIENTO DE MODELOS - NUEVA FUNCIÓN\n",
    "# ============================================================================\n",
    "\n",
    "def discover_models(models_dir=\"models\"):\n",
    "    \"\"\"Descubre modelos en la estructura de directorios actual\"\"\"\n",
    "    log_with_time(f\"Descubriendo modelos en '{models_dir}'\")\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        log_with_time(f\"El directorio {models_dir} no existe\", \"ERROR\")\n",
    "        return []\n",
    "    \n",
    "    discovered_models = []\n",
    "    \n",
    "    # Buscar archivos .joblib y .pkl recursivamente\n",
    "    for root, dirs, files in os.walk(models_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.joblib', '.pkl')):\n",
    "                full_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extraer información del path y nombre del archivo\n",
    "                rel_path = os.path.relpath(full_path, models_dir)\n",
    "                path_parts = rel_path.split(os.sep)\n",
    "                \n",
    "                # Inferir algoritmo del directorio padre\n",
    "                algorithm = path_parts[0] if len(path_parts) > 1 else \"Unknown\"\n",
    "                \n",
    "                # Extraer encoding del nombre del archivo\n",
    "                filename_no_ext = os.path.splitext(file)[0]\n",
    "                \n",
    "                # Patrones para extraer encoding\n",
    "                # Formato esperado: [ALGO_]ENCODING_TIMESTAMP.ext\n",
    "                encoding = None\n",
    "                \n",
    "                # Intentar diferentes patrones\n",
    "                patterns = [\n",
    "                    r'^(?:' + re.escape(algorithm) + '_)?(.+?)_\\d{8}_\\d{6}$',  # ALGO_ENCODING_TIMESTAMP\n",
    "                    r'^(.+?)_\\d{8}_\\d{6}$',  # ENCODING_TIMESTAMP\n",
    "                    r'^(?:' + re.escape(algorithm) + '_)?(.+)$'  # Solo ENCODING o ALGO_ENCODING\n",
    "                ]\n",
    "                \n",
    "                for pattern in patterns:\n",
    "                    match = re.match(pattern, filename_no_ext, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        encoding = match.group(1)\n",
    "                        break\n",
    "                \n",
    "                if not encoding:\n",
    "                    # Fallback: usar todo el nombre menos timestamp si existe\n",
    "                    parts = filename_no_ext.split('_')\n",
    "                    if len(parts) >= 3 and parts[-2].isdigit() and parts[-1].isdigit():\n",
    "                        encoding = '_'.join(parts[:-2])\n",
    "                    else:\n",
    "                        encoding = filename_no_ext\n",
    "                \n",
    "                # Limpiar encoding\n",
    "                if encoding.startswith(f\"{algorithm}_\"):\n",
    "                    encoding = encoding[len(algorithm)+1:]\n",
    "                \n",
    "                model_info = {\n",
    "                    'model_name': algorithm,\n",
    "                    'encoding': encoding,\n",
    "                    'model_file': full_path,\n",
    "                    'filename': file,\n",
    "                    'relative_path': rel_path,\n",
    "                    'metrics': {}  # Vacío ya que no tenemos datos de entrenamiento\n",
    "                }\n",
    "                \n",
    "                discovered_models.append(model_info)\n",
    "                log_with_time(f\"  Encontrado: {algorithm} - {encoding} ({file})\")\n",
    "    \n",
    "    log_with_time(f\"Total de modelos descubiertos: {len(discovered_models)}\")\n",
    "    return discovered_models\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNÓSTICO DE MODELOS - MODIFICADO\n",
    "# ============================================================================\n",
    "\n",
    "def diagnose_model_files(models_dir=\"models\"):\n",
    "    \"\"\"Diagnostica archivos de modelos para detectar problemas\"\"\"\n",
    "    log_with_time(f\"Diagnosticando archivos de modelos en '{models_dir}'\")\n",
    "    \n",
    "    if not os.path.exists(models_dir):\n",
    "        log_with_time(f\"El directorio {models_dir} no existe\", \"ERROR\")\n",
    "        return None\n",
    "    \n",
    "    # Buscar archivos de modelos recursivamente\n",
    "    model_files = []\n",
    "    for root, dirs, files in os.walk(models_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.pkl', '.joblib')):\n",
    "                model_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if not model_files:\n",
    "        log_with_time(\"No se encontraron archivos de modelos\", \"WARNING\")\n",
    "        return None\n",
    "    \n",
    "    log_with_time(f\"Encontrados {len(model_files)} archivos de modelos\")\n",
    "    \n",
    "    diagnosis = {\n",
    "        'total_files': len(model_files),\n",
    "        'valid_files': [],\n",
    "        'corrupted_files': [],\n",
    "        'empty_files': [],\n",
    "        'file_details': {}\n",
    "    }\n",
    "    \n",
    "    for filepath in model_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        \n",
    "        file_info = {\n",
    "            'path': filepath,\n",
    "            'size_bytes': file_size,\n",
    "            'size_mb': file_size / (1024 * 1024),\n",
    "            'status': 'unknown'\n",
    "        }\n",
    "        \n",
    "        if file_size == 0:\n",
    "            file_info['status'] = 'empty'\n",
    "            diagnosis['empty_files'].append(filename)\n",
    "        else:\n",
    "            try:\n",
    "                # Verificar si se puede cargar\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    first_bytes = f.read(20)\n",
    "                    f.seek(0)\n",
    "                    \n",
    "                    # Verificar cabecera pickle/joblib\n",
    "                    if first_bytes.startswith(b'\\x80') or b'sklearn' in first_bytes:\n",
    "                        # Intentar cargar\n",
    "                        try:\n",
    "                            if filepath.endswith('.pkl'):\n",
    "                                model = pickle.load(f)\n",
    "                            else:\n",
    "                                model = joblib.load(filepath)\n",
    "                            \n",
    "                            file_info['status'] = 'valid'\n",
    "                            file_info['model_type'] = type(model).__name__\n",
    "                            diagnosis['valid_files'].append(filename)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            file_info['status'] = 'corrupted'\n",
    "                            file_info['error'] = str(e)\n",
    "                            diagnosis['corrupted_files'].append(filename)\n",
    "                    else:\n",
    "                        file_info['status'] = 'invalid_format'\n",
    "                        diagnosis['corrupted_files'].append(filename)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                file_info['status'] = 'error'\n",
    "                file_info['error'] = str(e)\n",
    "                diagnosis['corrupted_files'].append(filename)\n",
    "        \n",
    "        diagnosis['file_details'][filename] = file_info\n",
    "    \n",
    "    log_with_time(f\"Archivos válidos: {len(diagnosis['valid_files'])}\")\n",
    "    log_with_time(f\"Archivos corruptos: {len(diagnosis['corrupted_files'])}\")\n",
    "    log_with_time(f\"Archivos vacíos: {len(diagnosis['empty_files'])}\")\n",
    "    \n",
    "    return diagnosis\n",
    "\n",
    "# ============================================================================\n",
    "# CARGA DE DATOS - MODIFICADO\n",
    "# ============================================================================\n",
    "\n",
    "def load_evaluation_data():\n",
    "    \"\"\"Carga datos de evaluación desde múltiples fuentes posibles\"\"\"\n",
    "    \n",
    "    # Intentar cargar desde archivo principal\n",
    "    if os.path.exists(config.data_file):\n",
    "        try:\n",
    "            log_with_time(f\"Cargando datos desde {config.data_file}\")\n",
    "            data = pd.read_parquet(config.data_file)\n",
    "            log_with_time(f\"Datos cargados exitosamente: {len(data)} filas\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            log_with_time(f\"Error cargando {config.data_file}: {e}\", \"WARNING\")\n",
    "    \n",
    "    # Intentar archivos alternativos\n",
    "    for alt_file in config.alternative_data_files:\n",
    "        if os.path.exists(alt_file):\n",
    "            try:\n",
    "                log_with_time(f\"Intentando cargar desde {alt_file}\")\n",
    "                if alt_file.endswith('.csv'):\n",
    "                    data = pd.read_csv(alt_file)\n",
    "                elif alt_file.endswith('.parquet'):\n",
    "                    data = pd.read_parquet(alt_file)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                log_with_time(f\"Datos cargados exitosamente: {len(data)} filas\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                log_with_time(f\"Error cargando {alt_file}: {e}\", \"WARNING\")\n",
    "    \n",
    "    log_with_time(\"No se pudo cargar ningún archivo de datos\", \"ERROR\")\n",
    "    return None\n",
    "\n",
    "def create_class_mapping(evaluation_data):\n",
    "    \"\"\"Crea un mapeo de clases basado en los datos disponibles\"\"\"\n",
    "    \n",
    "    if 'clases_modelos' in evaluation_data.columns:\n",
    "        # Ya existe el mapeo numérico\n",
    "        if 'genus' in evaluation_data.columns:\n",
    "            # Crear mapeo desde genus\n",
    "            mapping_data = evaluation_data[['genus', 'clases_modelos']].drop_duplicates()\n",
    "            class_mapping = dict(zip(mapping_data['clases_modelos'], mapping_data['genus']))\n",
    "        else:\n",
    "            # Crear mapeo genérico\n",
    "            unique_classes = evaluation_data['clases_modelos'].unique()\n",
    "            class_mapping = {i: f\"Class_{i}\" for i in unique_classes}\n",
    "    elif 'genus' in evaluation_data.columns:\n",
    "        # Crear mapeo numérico\n",
    "        unique_genera = evaluation_data['genus'].unique()\n",
    "        genus_to_id = {genus: i for i, genus in enumerate(sorted(unique_genera))}\n",
    "        evaluation_data['clases_modelos'] = evaluation_data['genus'].map(genus_to_id)\n",
    "        class_mapping = {i: genus for genus, i in genus_to_id.items()}\n",
    "    else:\n",
    "        log_with_time(\"No se encontraron columnas de clase válidas\", \"ERROR\")\n",
    "        return None, evaluation_data\n",
    "    \n",
    "    log_with_time(f\"Mapeo de clases creado: {len(class_mapping)} clases\")\n",
    "    return class_mapping, evaluation_data\n",
    "\n",
    "# ============================================================================\n",
    "# CÁLCULO DE MÉTRICAS DETALLADAS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_detailed_metrics(y_true, y_pred, y_proba, class_names, timing_info=None):\n",
    "    \"\"\"Calcula métricas detalladas incluyendo por clase\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Métricas globales\n",
    "    metrics['global'] = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "        'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'precision_micro': precision_score(y_true, y_pred, average='micro'),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'recall_micro': recall_score(y_true, y_pred, average='micro'),\n",
    "        'n_samples': len(y_true),\n",
    "        'n_classes': len(np.unique(y_true))\n",
    "    }\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    # Métricas por clase\n",
    "    unique_classes = np.unique(y_true)\n",
    "    metrics['per_class'] = {}\n",
    "    \n",
    "    # Calcular métricas por clase usando sklearn\n",
    "    class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    for i, class_id in enumerate(unique_classes):\n",
    "        class_name = class_names.get(class_id, f\"Class_{class_id}\")\n",
    "        class_str = str(class_id)\n",
    "        \n",
    "        if class_str in class_report:\n",
    "            class_metrics = class_report[class_str]\n",
    "            \n",
    "            # Calcular especificidad manualmente\n",
    "            TP = cm[i, i] if i < cm.shape[0] and i < cm.shape[1] else 0\n",
    "            FP = np.sum(cm[:, i]) - TP if i < cm.shape[1] else 0\n",
    "            FN = np.sum(cm[i, :]) - TP if i < cm.shape[0] else 0\n",
    "            TN = np.sum(cm) - (TP + FP + FN)\n",
    "            \n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "            \n",
    "            metrics['per_class'][class_name] = {\n",
    "                'class_id': int(class_id),\n",
    "                'precision': class_metrics['precision'],\n",
    "                'recall': class_metrics['recall'],\n",
    "                'f1_score': class_metrics['f1-score'],\n",
    "                'support': int(class_metrics['support']),\n",
    "                'sensitivity': class_metrics['recall'],  # sensitivity = recall\n",
    "                'specificity': specificity,\n",
    "                'true_positives': int(TP),\n",
    "                'false_positives': int(FP),\n",
    "                'false_negatives': int(FN),\n",
    "                'true_negatives': int(TN)\n",
    "            }\n",
    "    \n",
    "    # ROC AUC por clase (si hay probabilidades)\n",
    "    if y_proba is not None:\n",
    "        metrics['roc_auc'] = {}\n",
    "        try:\n",
    "            for i, class_id in enumerate(unique_classes):\n",
    "                if i < y_proba.shape[1]:\n",
    "                    class_name = class_names.get(class_id, f\"Class_{class_id}\")\n",
    "                    y_true_binary = (y_true == class_id).astype(int)\n",
    "                    \n",
    "                    if len(np.unique(y_true_binary)) > 1:  # Verificar que hay ambas clases\n",
    "                        fpr, tpr, _ = roc_curve(y_true_binary, y_proba[:, i])\n",
    "                        roc_auc = auc(fpr, tpr)\n",
    "                        metrics['roc_auc'][class_name] = float(roc_auc)\n",
    "        except Exception as e:\n",
    "            log_with_time(f\"Error calculando ROC AUC: {e}\", \"WARNING\")\n",
    "    \n",
    "    # Información de timing si está disponible\n",
    "    if timing_info:\n",
    "        metrics['timing'] = timing_info\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUACIÓN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def load_model_safely(model_path):\n",
    "    \"\"\"Carga un modelo de forma segura con manejo de errores\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if model_path.endswith('.pkl'):\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        else:\n",
    "            model = joblib.load(model_path)\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        return model, load_time, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, 0, str(e)\n",
    "\n",
    "def evaluate_single_model(model_info, evaluation_data, class_mapping):\n",
    "    \"\"\"Evalúa un solo modelo de forma completa\"\"\"\n",
    "    \n",
    "    model_name = model_info['model_name']\n",
    "    encoding = model_info['encoding']\n",
    "    model_file = model_info['model_file']\n",
    "    \n",
    "    log_with_time(f\"Evaluando {model_name} - {encoding}\")\n",
    "    \n",
    "    # Inicializar resultado\n",
    "    result = {\n",
    "        'model_name': model_name,\n",
    "        'encoding': encoding,\n",
    "        'model_file': model_file,\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'test_metrics': model_info.get('metrics', {}),\n",
    "        'eval_metrics': {},\n",
    "        'class_metrics': {},\n",
    "        'timing_info': {},\n",
    "        'predictions': {},\n",
    "        'efficiency_score': 0,\n",
    "        'data_info': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Verificar que el encoding existe en los datos\n",
    "        if encoding not in evaluation_data.columns:\n",
    "            raise ValueError(f\"Encoding {encoding} no encontrado en datos de evaluación\")\n",
    "        \n",
    "        # Cargar modelo\n",
    "        log_with_time(f\"  Cargando modelo...\")\n",
    "        model, load_time, load_error = load_model_safely(model_file)\n",
    "        \n",
    "        if model is None:\n",
    "            raise ValueError(f\"Error cargando modelo: {load_error}\")\n",
    "        \n",
    "        result['timing_info']['model_load_time'] = load_time\n",
    "        \n",
    "        # Preparar datos\n",
    "        log_with_time(f\"  Preparando datos...\")\n",
    "        data_prep_start = time.time()\n",
    "        \n",
    "        X_eval = evaluation_data[encoding].tolist()\n",
    "        y_eval = evaluation_data['clases_modelos'].values\n",
    "        \n",
    "        # Filtrar valores None/NaN\n",
    "        valid_indices = [i for i, x in enumerate(X_eval) \n",
    "                        if x is not None and not (isinstance(x, (int, float)) and np.isnan(x))]\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            raise ValueError(\"No hay datos válidos para evaluación\")\n",
    "        \n",
    "        X_eval_clean = [X_eval[i] for i in valid_indices]\n",
    "        y_eval_clean = y_eval[valid_indices]\n",
    "        \n",
    "        # Convertir a numpy array\n",
    "        X_eval_array = np.array(X_eval_clean)\n",
    "        \n",
    "        data_prep_time = time.time() - data_prep_start\n",
    "        result['timing_info']['data_prep_time'] = data_prep_time\n",
    "        \n",
    "        # Información de datos\n",
    "        result['data_info'] = {\n",
    "            'original_samples': len(X_eval),\n",
    "            'valid_samples': len(X_eval_clean),\n",
    "            'feature_dimensions': X_eval_array.shape[1] if len(X_eval_array.shape) > 1 else 1,\n",
    "            'classes_present': len(np.unique(y_eval_clean)),\n",
    "            'data_reduction_ratio': len(X_eval_clean) / len(X_eval)\n",
    "        }\n",
    "        \n",
    "        # Hacer predicciones\n",
    "        log_with_time(f\"  Realizando predicciones...\")\n",
    "        prediction_start = time.time()\n",
    "        \n",
    "        y_pred = model.predict(X_eval_array)\n",
    "        \n",
    "        # Verificar si el modelo tiene predict_proba\n",
    "        y_proba = None\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_proba = model.predict_proba(X_eval_array)\n",
    "        except Exception as e:\n",
    "            log_with_time(f\"  Warning: No se pudieron obtener probabilidades: {e}\", \"WARNING\")\n",
    "        \n",
    "        prediction_time = time.time() - prediction_start\n",
    "        result['timing_info']['prediction_time'] = prediction_time\n",
    "        result['timing_info']['total_time'] = load_time + data_prep_time + prediction_time\n",
    "        \n",
    "        # Calcular métricas detalladas\n",
    "        log_with_time(f\"  Calculando métricas...\")\n",
    "        metrics_start = time.time()\n",
    "        \n",
    "        detailed_metrics = calculate_detailed_metrics(\n",
    "            y_eval_clean, y_pred, y_proba, class_mapping, result['timing_info']\n",
    "        )\n",
    "        \n",
    "        metrics_time = time.time() - metrics_start\n",
    "        result['timing_info']['metrics_calc_time'] = metrics_time\n",
    "        \n",
    "        # Almacenar resultados\n",
    "        result['eval_metrics'] = detailed_metrics['global']\n",
    "        result['class_metrics'] = detailed_metrics['per_class']\n",
    "        result['confusion_matrix'] = detailed_metrics['confusion_matrix']\n",
    "        \n",
    "        if 'roc_auc' in detailed_metrics:\n",
    "            result['roc_auc'] = detailed_metrics['roc_auc']\n",
    "        \n",
    "        # Predicciones detalladas\n",
    "        if config.save_individual_predictions:\n",
    "            result['predictions'] = {\n",
    "                'y_true': y_eval_clean.tolist(),\n",
    "                'y_pred': y_pred.tolist(),\n",
    "                'y_proba': y_proba.tolist() if y_proba is not None else None,\n",
    "                'correct_predictions': (y_eval_clean == y_pred).tolist()\n",
    "            }\n",
    "        \n",
    "        # Calcular efficiency score\n",
    "        if result['timing_info']['total_time'] > 0:\n",
    "            result['efficiency_score'] = result['eval_metrics']['accuracy'] / result['timing_info']['total_time']\n",
    "        \n",
    "        result['success'] = True\n",
    "        \n",
    "        log_with_time(f\"  Completado - Accuracy: {result['eval_metrics']['accuracy']:.4f}\")\n",
    "        \n",
    "        # Liberar memoria si está configurado\n",
    "        if config.memory_efficient:\n",
    "            del model, X_eval_array, y_pred\n",
    "            if y_proba is not None:\n",
    "                del y_proba\n",
    "            gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        log_with_time(f\"  Error: {e}\", \"ERROR\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# ANÁLISIS ESTADÍSTICO\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_statistical_analysis(results_df):\n",
    "    \"\"\"Calcula análisis estadístico de los resultados\"\"\"\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Estadísticas descriptivas por métrica\n",
    "    numeric_columns = ['eval_accuracy', 'eval_f1_weighted', 'eval_f1_macro', \n",
    "                      'total_time', 'efficiency_score']\n",
    "    \n",
    "    available_columns = [col for col in numeric_columns if col in results_df.columns]\n",
    "    \n",
    "    if available_columns:\n",
    "        analysis['descriptive_stats'] = {}\n",
    "        for col in available_columns:\n",
    "            analysis['descriptive_stats'][col] = {\n",
    "                'mean': float(results_df[col].mean()),\n",
    "                'std': float(results_df[col].std()),\n",
    "                'min': float(results_df[col].min()),\n",
    "                'max': float(results_df[col].max()),\n",
    "                'median': float(results_df[col].median()),\n",
    "                'q25': float(results_df[col].quantile(0.25)),\n",
    "                'q75': float(results_df[col].quantile(0.75))\n",
    "            }\n",
    "    \n",
    "    # Correlaciones entre métricas - CORREGIDO\n",
    "    if len(available_columns) > 1:\n",
    "        correlation_matrix = results_df[available_columns].corr()\n",
    "        # Convertir matriz de correlación a formato JSON-serializable\n",
    "        analysis['correlations'] = {}\n",
    "        for col1 in correlation_matrix.columns:\n",
    "            analysis['correlations'][str(col1)] = {}\n",
    "            for col2 in correlation_matrix.columns:\n",
    "                analysis['correlations'][str(col1)][str(col2)] = float(correlation_matrix.loc[col1, col2])\n",
    "    \n",
    "    # Rankings\n",
    "    if 'eval_accuracy' in results_df.columns:\n",
    "        analysis['rankings'] = {\n",
    "            'by_accuracy': results_df.nlargest(10, 'eval_accuracy')[\n",
    "                ['model_name', 'encoding', 'eval_accuracy']\n",
    "            ].to_dict('records'),\n",
    "            'by_efficiency': results_df.nlargest(10, 'efficiency_score')[\n",
    "                ['model_name', 'encoding', 'efficiency_score']\n",
    "            ].to_dict('records') if 'efficiency_score' in results_df.columns else []\n",
    "        }\n",
    "    \n",
    "    # Análisis por algoritmo - CORREGIDO\n",
    "    if 'model_name' in results_df.columns:\n",
    "        algorithm_stats = results_df.groupby('model_name').agg({\n",
    "            'eval_accuracy': ['mean', 'std', 'count'],\n",
    "            'total_time': ['mean', 'std'] if 'total_time' in results_df.columns else ['count']\n",
    "        }).round(4)\n",
    "        \n",
    "        # Convertir MultiIndex a formato JSON-serializable\n",
    "        analysis['by_algorithm'] = {}\n",
    "        for algorithm in algorithm_stats.index:\n",
    "            analysis['by_algorithm'][str(algorithm)] = {}\n",
    "            for col in algorithm_stats.columns:\n",
    "                # col es una tupla (métrica, estadística)\n",
    "                metric, stat = col\n",
    "                key = f\"{metric}_{stat}\"\n",
    "                analysis['by_algorithm'][str(algorithm)][key] = float(algorithm_stats.loc[algorithm, col])\n",
    "    \n",
    "    # Análisis por encoding - CORREGIDO\n",
    "    if 'encoding' in results_df.columns:\n",
    "        encoding_stats = results_df.groupby('encoding').agg({\n",
    "            'eval_accuracy': ['mean', 'std', 'count'],\n",
    "            'total_time': ['mean', 'std'] if 'total_time' in results_df.columns else ['count']\n",
    "        }).round(4)\n",
    "        \n",
    "        # Convertir MultiIndex a formato JSON-serializable\n",
    "        analysis['by_encoding'] = {}\n",
    "        for encoding in encoding_stats.index:\n",
    "            analysis['by_encoding'][str(encoding)] = {}\n",
    "            for col in encoding_stats.columns:\n",
    "                # col es una tupla (métrica, estadística)\n",
    "                metric, stat = col\n",
    "                key = f\"{metric}_{stat}\"\n",
    "                analysis['by_encoding'][str(encoding)][key] = float(encoding_stats.loc[encoding, col])\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def convert_numpy_and_types(obj):\n",
    "    \"\"\"Función mejorada para convertir tipos no serializables\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, tuple):\n",
    "        # Convertir tuplas a listas\n",
    "        return [convert_numpy_and_types(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        # Asegurar que las claves sean strings\n",
    "        return {str(k): convert_numpy_and_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_and_types(item) for item in obj]\n",
    "    elif isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "# ============================================================================\n",
    "# GUARDADO DE RESULTADOS\n",
    "# ============================================================================\n",
    "\n",
    "def save_detailed_results(all_results, statistical_analysis, timestamp):\n",
    "    \"\"\"Guarda todos los resultados en múltiples formatos\"\"\"\n",
    "    \n",
    "    # CORRECCIÓN: Crear directorio directamente\n",
    "    os.makedirs(config.results_dir, exist_ok=True)\n",
    "    \n",
    "    # Crear DataFrame principal\n",
    "    main_data = []\n",
    "    for result in all_results:\n",
    "        if result['success']:\n",
    "            row = {\n",
    "                'model_name': result['model_name'],\n",
    "                'encoding': result['encoding'],\n",
    "                'eval_accuracy': result['eval_metrics'].get('accuracy', 0),\n",
    "                'eval_f1_weighted': result['eval_metrics'].get('f1_weighted', 0),\n",
    "                'eval_f1_macro': result['eval_metrics'].get('f1_macro', 0),\n",
    "                'eval_precision_weighted': result['eval_metrics'].get('precision_weighted', 0),\n",
    "                'eval_recall_weighted': result['eval_metrics'].get('recall_weighted', 0),\n",
    "                'n_samples': result['eval_metrics'].get('n_samples', 0),\n",
    "                'n_classes': result['eval_metrics'].get('n_classes', 0),\n",
    "                'total_time': result['timing_info'].get('total_time', 0),\n",
    "                'prediction_time': result['timing_info'].get('prediction_time', 0),\n",
    "                'model_load_time': result['timing_info'].get('model_load_time', 0),\n",
    "                'efficiency_score': result['efficiency_score'],\n",
    "                'data_reduction_ratio': result['data_info'].get('data_reduction_ratio', 1.0),\n",
    "                'model_file': result['model_file']\n",
    "            }\n",
    "            \n",
    "            main_data.append(row)\n",
    "    \n",
    "    df_main = pd.DataFrame(main_data)\n",
    "    \n",
    "    if df_main.empty:\n",
    "        log_with_time(\"No hay resultados exitosos para guardar\", \"WARNING\")\n",
    "        return\n",
    "    \n",
    "    # Guardar CSV principal\n",
    "    csv_path = os.path.join(config.results_dir, f\"evaluation_results_{timestamp}.csv\")\n",
    "    df_main.to_csv(csv_path, index=False)\n",
    "    log_with_time(f\"Resultados principales guardados: {csv_path}\")\n",
    "    \n",
    "    # Guardar Excel con múltiples hojas\n",
    "    excel_path = os.path.join(config.results_dir, f\"evaluation_complete_{timestamp}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Hoja principal\n",
    "        df_main.to_excel(writer, sheet_name='Resumen', index=False)\n",
    "        \n",
    "        # Hoja con métricas por clase\n",
    "        class_data = []\n",
    "        for result in all_results:\n",
    "            if result['success'] and result['class_metrics']:\n",
    "                for class_name, metrics in result['class_metrics'].items():\n",
    "                    row = {\n",
    "                        'model_name': result['model_name'],\n",
    "                        'encoding': result['encoding'],\n",
    "                        'class_name': class_name,\n",
    "                        'class_id': metrics.get('class_id', ''),\n",
    "                        **metrics\n",
    "                    }\n",
    "                    class_data.append(row)\n",
    "        \n",
    "        if class_data:\n",
    "            df_class = pd.DataFrame(class_data)\n",
    "            df_class.to_excel(writer, sheet_name='Metricas_por_Clase', index=False)\n",
    "        \n",
    "        # Hoja con información de timing\n",
    "        timing_data = []\n",
    "        for result in all_results:\n",
    "            if result['success'] and result['timing_info']:\n",
    "                row = {\n",
    "                    'model_name': result['model_name'],\n",
    "                    'encoding': result['encoding'],\n",
    "                    **result['timing_info']\n",
    "                }\n",
    "                timing_data.append(row)\n",
    "        \n",
    "        if timing_data:\n",
    "            df_timing = pd.DataFrame(timing_data)\n",
    "            df_timing.to_excel(writer, sheet_name='Tiempos_Ejecucion', index=False)\n",
    "        \n",
    "        # Hoja con análisis estadístico\n",
    "        if statistical_analysis:\n",
    "            # Convertir análisis estadístico a formato tabular\n",
    "            stats_rows = []\n",
    "            if 'descriptive_stats' in statistical_analysis:\n",
    "                for metric, stats in statistical_analysis['descriptive_stats'].items():\n",
    "                    for stat_name, value in stats.items():\n",
    "                        stats_rows.append({\n",
    "                            'metric': metric,\n",
    "                            'statistic': stat_name,\n",
    "                            'value': value\n",
    "                        })\n",
    "            \n",
    "            if stats_rows:\n",
    "                df_stats = pd.DataFrame(stats_rows)\n",
    "                df_stats.to_excel(writer, sheet_name='Analisis_Estadistico', index=False)\n",
    "    \n",
    "    log_with_time(f\"Excel completo guardado: {excel_path}\")\n",
    "    \n",
    "    # Guardar resultados completos en JSON - CORREGIDO\n",
    "    json_path = os.path.join(config.results_dir, f\"evaluation_detailed_{timestamp}.json\")\n",
    "    \n",
    "    # Preparar datos para JSON\n",
    "    json_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'config': vars(config),\n",
    "        'results': all_results,\n",
    "        'statistical_analysis': statistical_analysis,\n",
    "        'summary': {\n",
    "            'total_models': len(all_results),\n",
    "            'successful_evaluations': len([r for r in all_results if r['success']]),\n",
    "            'failed_evaluations': len([r for r in all_results if not r['success']])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convertir todos los tipos no serializables\n",
    "    json_data = convert_numpy_and_types(json_data)\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(json_data, f, indent=2)\n",
    "        log_with_time(f\"JSON detallado guardado: {json_path}\")\n",
    "    except Exception as e:\n",
    "        log_with_time(f\"Error guardando JSON: {e}\", \"WARNING\")\n",
    "        # Intentar guardar versión simplificada\n",
    "        simplified_data = {\n",
    "            'timestamp': timestamp,\n",
    "            'summary': json_data['summary'],\n",
    "            'descriptive_stats': statistical_analysis.get('descriptive_stats', {}),\n",
    "            'rankings': statistical_analysis.get('rankings', {})\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            json_simplified_path = os.path.join(config.results_dir, f\"evaluation_summary_{timestamp}.json\")\n",
    "            with open(json_simplified_path, 'w') as f:\n",
    "                json.dump(simplified_data, f, indent=2)\n",
    "            log_with_time(f\"JSON simplificado guardado: {json_simplified_path}\")\n",
    "            json_path = json_simplified_path\n",
    "        except Exception as e2:\n",
    "            log_with_time(f\"Error guardando JSON simplificado: {e2}\", \"ERROR\")\n",
    "            json_path = None\n",
    "    \n",
    "    return {\n",
    "        'csv_path': csv_path,\n",
    "        'excel_path': excel_path,\n",
    "        'json_path': json_path,\n",
    "        'main_dataframe': df_main\n",
    "    }\n",
    "    \n",
    "# ============================================================================\n",
    "# FUNCIÓN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal de evaluación numérica\"\"\"\n",
    "    \n",
    "    log_with_time(\"Iniciando evaluación numérica completa...\")\n",
    "    \n",
    "    # Crear directorio de resultados\n",
    "    os.makedirs(config.results_dir, exist_ok=True)\n",
    "    # ========================================================================\n",
    "    # DESCUBRIMIENTO Y VERIFICACIÓN DE MODELOS\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Descubriendo modelos...\")\n",
    "    \n",
    "    discovered_models = discover_models(config.models_dir)\n",
    "    if not discovered_models:\n",
    "        log_with_time(\"No se encontraron modelos\", \"ERROR\")\n",
    "        return False\n",
    "    \n",
    "    # Diagnosticar modelos\n",
    "    diagnosis = diagnose_model_files(config.models_dir)\n",
    "    if diagnosis is None or len(diagnosis['valid_files']) == 0:\n",
    "        log_with_time(\"No se encontraron modelos válidos\", \"ERROR\")\n",
    "        return False\n",
    "    \n",
    "    log_with_time(f\"Modelos válidos encontrados: {len(diagnosis['valid_files'])}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CARGAR DATOS\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Cargando datos de evaluación...\")\n",
    "    \n",
    "    evaluation_data = load_evaluation_data()\n",
    "    if evaluation_data is None:\n",
    "        return False\n",
    "    \n",
    "    # Crear mapeo de clases\n",
    "    class_mapping, evaluation_data = create_class_mapping(evaluation_data)\n",
    "    if class_mapping is None:\n",
    "        return False\n",
    "    \n",
    "    log_with_time(f\"Clases disponibles: {len(class_mapping)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EVALUACIÓN DE MODELOS\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Iniciando evaluación de modelos...\")\n",
    "    \n",
    "    all_results = []\n",
    "    successful_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    total_models = len(discovered_models)\n",
    "    \n",
    "    for i, model_info in enumerate(discovered_models):\n",
    "        log_with_time(f\"Progreso: {i+1}/{total_models}\")\n",
    "        \n",
    "        # Verificar que el archivo de modelo existe\n",
    "        if not os.path.exists(model_info['model_file']):\n",
    "            log_with_time(f\"Archivo de modelo no encontrado: {model_info['model_file']}\", \"WARNING\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        result = evaluate_single_model(model_info, evaluation_data, class_mapping)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            successful_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "        \n",
    "        # Liberar memoria cada ciertos modelos\n",
    "        if config.memory_efficient and i % config.max_models_in_memory == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    log_with_time(f\"Evaluación completada: {successful_count} exitosos, {failed_count} fallidos\")\n",
    "    \n",
    "    if successful_count == 0:\n",
    "        log_with_time(\"No se evaluaron modelos exitosamente\", \"ERROR\")\n",
    "        return False\n",
    "    \n",
    "    # ========================================================================\n",
    "    # ANÁLISIS ESTADÍSTICO\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Calculando análisis estadístico...\")\n",
    "    \n",
    "    # Crear DataFrame para análisis\n",
    "    successful_results = [r for r in all_results if r['success']]\n",
    "    \n",
    "    analysis_data = []\n",
    "    for result in successful_results:\n",
    "        row = {\n",
    "            'model_name': result['model_name'],\n",
    "            'encoding': result['encoding'],\n",
    "            'eval_accuracy': result['eval_metrics'].get('accuracy', 0),\n",
    "            'eval_f1_weighted': result['eval_metrics'].get('f1_weighted', 0),\n",
    "            'eval_f1_macro': result['eval_metrics'].get('f1_macro', 0),\n",
    "            'total_time': result['timing_info'].get('total_time', 0),\n",
    "            'efficiency_score': result['efficiency_score']\n",
    "        }\n",
    "        analysis_data.append(row)\n",
    "    \n",
    "    df_for_analysis = pd.DataFrame(analysis_data)\n",
    "    statistical_analysis = calculate_statistical_analysis(df_for_analysis)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # GUARDAR RESULTADOS\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Guardando resultados...\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    saved_files = save_detailed_results(all_results, statistical_analysis, timestamp)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESUMEN FINAL\n",
    "    # ========================================================================\n",
    "    log_with_time(\"Generando resumen final...\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUACIÓN NUMÉRICA COMPLETADA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Modelos evaluados exitosamente: {successful_count}\")\n",
    "    print(f\"Modelos fallidos: {failed_count}\")\n",
    "    print(f\"Total de modelos procesados: {len(all_results)}\")\n",
    "    \n",
    "    if saved_files and 'main_dataframe' in saved_files:\n",
    "        df = saved_files['main_dataframe']\n",
    "        if not df.empty:\n",
    "            print(f\"\\nMEJORES RESULTADOS:\")\n",
    "            print(f\"Mejor accuracy: {df['eval_accuracy'].max():.4f}\")\n",
    "            print(f\"Accuracy promedio: {df['eval_accuracy'].mean():.4f}\")\n",
    "            print(f\"Mejor eficiencia: {df['efficiency_score'].max():.4f}\")\n",
    "            \n",
    "            best_model = df.loc[df['eval_accuracy'].idxmax()]\n",
    "            print(f\"\\nMEJOR MODELO:\")\n",
    "            print(f\"  Algoritmo: {best_model['model_name']}\")\n",
    "            print(f\"  Encoding: {best_model['encoding']}\")\n",
    "            print(f\"  Accuracy: {best_model['eval_accuracy']:.4f}\")\n",
    "            print(f\"  Tiempo: {best_model['total_time']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nARCHIVOS GENERADOS:\")\n",
    "    for key, path in saved_files.items():\n",
    "        if key != 'main_dataframe':\n",
    "            print(f\"  {key}: {path}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUTAR SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Iniciando evaluación numérica...\")\n",
    "    \n",
    "    try:\n",
    "        success = main()\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\nEVALUACIÓN NUMÉRICA COMPLETADA CON ÉXITO!\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            print(\"\\nLA EVALUACIÓN NUMÉRICA FALLÓ\")\n",
    "            exit(1)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEvaluación interrumpida por el usuario\")\n",
    "        exit(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR FATAL: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
